{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a8289fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:48:47.504505Z",
     "start_time": "2024-03-24T21:48:38.877816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "34 47 41 %\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def extract_numbers_from_images(folder_path):\n",
    "    # Load the digit recognition model\n",
    "    model = load_model(\"model/digits.h5\")  # Path to your trained digit recognition model\n",
    "\n",
    "    numbers = np.zeros((9, 9), dtype=int)\n",
    "\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            img_path = os.path.join(folder_path, f\"cell[{i}][{j}].jpg\")\n",
    "            if os.path.exists(img_path):\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    # Preprocess the image (resize, normalize, etc.) as needed\n",
    "                    img = preprocess_image(img)\n",
    "                    # Make prediction using the model\n",
    "                    prediction = model.predict(np.expand_dims(img, axis=0))\n",
    "                    # Assign the predicted digit to the corresponding position in the array\n",
    "                    numbers[i][j] = np.argmax(prediction)\n",
    "\n",
    "    return numbers\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Preprocess the image (resize, normalize, etc.) as needed\n",
    "    # Example: Resize the image to 28x28 and normalize pixel values to [0, 1]\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    return img\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"Cells/\"\n",
    "output_array = extract_numbers_from_images(folder_path)\n",
    "#print(output_array)\n",
    "\n",
    "\n",
    "cell = [\n",
    "    [5,3,0, 0,7,0, 0,0,0],\n",
    "    [6,0,0, 1,9,5, 0,0,0],\n",
    "    [0,9,8, 0,0,0, 0,6,0],\n",
    "    \n",
    "    [8,0,0, 0,6,0, 0,0,3],\n",
    "    [4,0,0, 8,0,3, 0,0,1],\n",
    "    [7,0,0, 0,2,0, 0,0,6],\n",
    "    \n",
    "    [0,6,0, 0,0,0, 2,8,0],\n",
    "    [0,0,0, 4,1,9, 0,0,5],\n",
    "    [0,0,0, 0,8,0, 0,7,9]\n",
    "]\n",
    "\n",
    "correct = 0\n",
    "wrong = 81\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        if (cell[i][j] == output_array [i][j]):\n",
    "            correct += 1\n",
    "            wrong -= 1\n",
    "            \n",
    "            \n",
    "print(correct,wrong,int((correct/81)*100) , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "533b4f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:55:18.352098Z",
     "start_time": "2024-03-24T21:55:10.274922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "59 22 72 %\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def extract_numbers_from_images(folder_path):\n",
    "    # Load the digit recognition model\n",
    "    model = load_model(\"Newfolder/best_model_72.h5\")  # Path to your trained digit recognition model\n",
    "\n",
    "    numbers = np.zeros((9, 9), dtype=int)\n",
    "\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            img_path = os.path.join(folder_path, f\"cell[{i}][{j}].jpg\")\n",
    "            if os.path.exists(img_path):\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    # Preprocess the image (resize, normalize, etc.) as needed\n",
    "                    img = preprocess_image(img)\n",
    "                    # Make prediction using the model\n",
    "                    prediction = model.predict(np.expand_dims(img, axis=0))\n",
    "                    # Assign the predicted digit to the corresponding position in the array\n",
    "                    numbers[i][j] = np.argmax(prediction)\n",
    "\n",
    "    return numbers\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Preprocess the image (resize, normalize, etc.) as needed\n",
    "    # Example: Resize the image to 28x28 and normalize pixel values to [0, 1]\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    return img\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"Cells/\"\n",
    "output_array = extract_numbers_from_images(folder_path)\n",
    "#print(output_array)\n",
    "\n",
    "\n",
    "cell = [\n",
    "    [5,3,0, 0,7,0, 0,0,0],\n",
    "    [6,0,0, 1,9,5, 0,0,0],\n",
    "    [0,9,8, 0,0,0, 0,6,0],\n",
    "    \n",
    "    [8,0,0, 0,6,0, 0,0,3],\n",
    "    [4,0,0, 8,0,3, 0,0,1],\n",
    "    [7,0,0, 0,2,0, 0,0,6],\n",
    "    \n",
    "    [0,6,0, 0,0,0, 2,8,0],\n",
    "    [0,0,0, 4,1,9, 0,0,5],\n",
    "    [0,0,0, 0,8,0, 0,7,9]\n",
    "]\n",
    "\n",
    "correct = 0\n",
    "wrong = 81\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        if (cell[i][j] == output_array [i][j]):\n",
    "            correct += 1\n",
    "            wrong -= 1\n",
    "            \n",
    "            \n",
    "print(correct,wrong,int((correct/81)*100) , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aac52346",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:38:31.000605Z",
     "start_time": "2024-03-24T21:38:30.993666Z"
    }
   },
   "outputs": [],
   "source": [
    "cell = [\n",
    "    [5,3,0, 0,7,0, 0,0,0],\n",
    "    [6,0,0, 1,9,5, 0,0,0],\n",
    "    [0,9,8, 0,0,0, 0,6,0],\n",
    "    \n",
    "    [8,0,0, 0,6,0, 0,0,3],\n",
    "    [4,0,0, 8,0,3, 0,0,1],\n",
    "    [7,0,0, 0,2,0, 0,0,6],\n",
    "    \n",
    "    [0,6,0, 0,0,0, 2,8,0],\n",
    "    [0,0,0, 4,1,9, 0,0,5],\n",
    "    [0,0,0, 0,8,0, 0,7,9]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d055e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:46:29.721847Z",
     "start_time": "2024-03-24T21:43:00.338916Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    # Reshape images to 28x28x1 (adding a channel dimension)\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def build_model(input_shape=(28, 28, 1), num_classes=10):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = build_model()\n",
    "    # Use early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "    return model\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Resize the image to 28x28 and normalize pixel values to [0, 1]\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    return img\n",
    "\n",
    "def extract_numbers_from_images(folder_path, model):\n",
    "    numbers = np.zeros((9, 9), dtype=int)\n",
    "\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            img_path = os.path.join(folder_path, f\"cell[{i}][{j}].jpg\")\n",
    "            if os.path.exists(img_path):\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    img = preprocess_image(img)\n",
    "                    # Make prediction using the model\n",
    "                    prediction = model.predict(np.expand_dims(img, axis=0))\n",
    "                    # Assign the predicted digit to the corresponding position in the array\n",
    "                    numbers[i][j] = np.argmax(prediction)\n",
    "\n",
    "    return numbers\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "X_train, X_test, y_train, y_test = load_data()\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "# Train the model\n",
    "model = train_model(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"Cells/\"\n",
    "output_array = extract_numbers_from_images(folder_path, model)\n",
    "#print(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a239355b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:53:19.651339Z",
     "start_time": "2024-03-24T21:53:19.641052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 22 72 %\n"
     ]
    }
   ],
   "source": [
    "cell = [\n",
    "    [5,3,0, 0,7,0, 0,0,0],\n",
    "    [6,0,0, 1,9,5, 0,0,0],\n",
    "    [0,9,8, 0,0,0, 0,6,0],\n",
    "    \n",
    "    [8,0,0, 0,6,0, 0,0,3],\n",
    "    [4,0,0, 8,0,3, 0,0,1],\n",
    "    [7,0,0, 0,2,0, 0,0,6],\n",
    "    \n",
    "    [0,6,0, 0,0,0, 2,8,0],\n",
    "    [0,0,0, 4,1,9, 0,0,5],\n",
    "    [0,0,0, 0,8,0, 0,7,9]\n",
    "]\n",
    "\n",
    "correct = 0\n",
    "wrong = 81\n",
    "for i in range(9):\n",
    "    for j in range(9):\n",
    "        if (cell[i][j] == output_array [i][j]):\n",
    "            correct += 1\n",
    "            wrong -= 1\n",
    "            \n",
    "            \n",
    "print(correct,wrong,int((correct/81)*100) , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1495dac7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:48:03.485689Z",
     "start_time": "2024-03-24T21:48:03.478926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 %\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "519ed496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:52:57.020749Z",
     "start_time": "2024-03-24T21:49:59.307498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 [==============================] - 17s 36ms/step - loss: 0.3421 - accuracy: 0.8949 - val_loss: 0.0732 - val_accuracy: 0.9787\n",
      "Epoch 2/20\n",
      "  5/422 [..............................] - ETA: 12s - loss: 0.1363 - accuracy: 0.9641"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\SETUP\\Anaconda\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422/422 [==============================] - 14s 34ms/step - loss: 0.1117 - accuracy: 0.9676 - val_loss: 0.0501 - val_accuracy: 0.9852\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 14s 34ms/step - loss: 0.0805 - accuracy: 0.9762 - val_loss: 0.0442 - val_accuracy: 0.9880\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 14s 34ms/step - loss: 0.0659 - accuracy: 0.9801 - val_loss: 0.0410 - val_accuracy: 0.9892\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 15s 35ms/step - loss: 0.0573 - accuracy: 0.9829 - val_loss: 0.0396 - val_accuracy: 0.9882\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 14s 32ms/step - loss: 0.0490 - accuracy: 0.9851 - val_loss: 0.0371 - val_accuracy: 0.9887\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 14s 33ms/step - loss: 0.0425 - accuracy: 0.9873 - val_loss: 0.0339 - val_accuracy: 0.9893\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 14s 33ms/step - loss: 0.0407 - accuracy: 0.9875 - val_loss: 0.0307 - val_accuracy: 0.9900\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 0.0346 - accuracy: 0.9895 - val_loss: 0.0335 - val_accuracy: 0.9900\n",
      "Epoch 10/20\n",
      "422/422 [==============================] - 17s 39ms/step - loss: 0.0299 - accuracy: 0.9908 - val_loss: 0.0325 - val_accuracy: 0.9898\n",
      "Epoch 11/20\n",
      "422/422 [==============================] - 14s 33ms/step - loss: 0.0280 - accuracy: 0.9913 - val_loss: 0.0325 - val_accuracy: 0.9912\n",
      "1/1 [==============================] - 0s 336ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "[[8 2 0 0 0 0 0 0 0]\n",
      " [8 0 0 0 8 8 0 0 0]\n",
      " [0 8 6 0 0 0 0 6 0]\n",
      " [8 0 0 0 6 0 0 0 8]\n",
      " [8 0 0 6 0 2 0 0 0]\n",
      " [0 0 0 0 2 0 0 0 6]\n",
      " [0 6 0 0 0 0 2 6 0]\n",
      " [0 0 0 8 0 8 0 0 6]\n",
      " [0 0 0 0 8 0 0 8 0]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    # Reshape images to 28x28x1 (adding a channel dimension)\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def build_model(input_shape=(28, 28, 1), num_classes=10):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = build_model()\n",
    "    # Use early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    # Save the best model during training\n",
    "    checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stopping, checkpoint])\n",
    "    return model\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Resize the image to 28x28 and normalize pixel values to [0, 1]\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    return img\n",
    "\n",
    "def extract_numbers_from_images(folder_path, model):\n",
    "    numbers = np.zeros((9, 9), dtype=int)\n",
    "\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            img_path = os.path.join(folder_path, f\"cell[{i}][{j}].jpg\")\n",
    "            if os.path.exists(img_path):\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    img = preprocess_image(img)\n",
    "                    # Make prediction using the model\n",
    "                    prediction = model.predict(np.expand_dims(img, axis=0))\n",
    "                    # Assign the predicted digit to the corresponding position in the array\n",
    "                    numbers[i][j] = np.argmax(prediction)\n",
    "\n",
    "    return numbers\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "X_train, X_test, y_train, y_test = load_data()\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "# Train the model\n",
    "model = train_model(X_train, y_train, X_val, y_val)\n",
    "# Load the best model\n",
    "best_model = load_model(\"best_model.h5\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"Cells/\"\n",
    "output_array = extract_numbers_from_images(folder_path, best_model)\n",
    "print(output_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "494b6d01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T22:03:34.810888Z",
     "start_time": "2024-03-24T22:00:44.063524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 [==============================] - 18s 37ms/step - loss: 0.3198 - accuracy: 0.9021 - val_loss: 0.0799 - val_accuracy: 0.9760\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 18s 41ms/step - loss: 0.1014 - accuracy: 0.9704 - val_loss: 0.0592 - val_accuracy: 0.9810\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 17s 41ms/step - loss: 0.0733 - accuracy: 0.9781 - val_loss: 0.0443 - val_accuracy: 0.9855\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 14s 34ms/step - loss: 0.0618 - accuracy: 0.9815 - val_loss: 0.0364 - val_accuracy: 0.9878\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 16s 38ms/step - loss: 0.0519 - accuracy: 0.9844 - val_loss: 0.0358 - val_accuracy: 0.9887\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 14s 32ms/step - loss: 0.0454 - accuracy: 0.9862 - val_loss: 0.0342 - val_accuracy: 0.9888\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 13s 31ms/step - loss: 0.0388 - accuracy: 0.9881 - val_loss: 0.0376 - val_accuracy: 0.9895\n",
      "Epoch 8/20\n",
      "422/422 [==============================] - 14s 33ms/step - loss: 0.0355 - accuracy: 0.9890 - val_loss: 0.0303 - val_accuracy: 0.9912\n",
      "Epoch 9/20\n",
      "422/422 [==============================] - 15s 34ms/step - loss: 0.0320 - accuracy: 0.9898 - val_loss: 0.0339 - val_accuracy: 0.9893\n",
      "Epoch 10/20\n",
      "422/422 [==============================] - 16s 38ms/step - loss: 0.0284 - accuracy: 0.9910 - val_loss: 0.0372 - val_accuracy: 0.9893\n",
      "Epoch 11/20\n",
      "422/422 [==============================] - 15s 36ms/step - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.0313 - val_accuracy: 0.9918\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\thresh.cpp:1674: error: (-215:Assertion failed) src.type() == CV_8UC1 in function 'cv::adaptiveThreshold'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     78\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCells/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 79\u001b[0m output_array \u001b[38;5;241m=\u001b[39m extract_numbers_from_images(folder_path, best_model)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_array)\n",
      "Cell \u001b[1;32mIn[34], line 60\u001b[0m, in \u001b[0;36mextract_numbers_from_images\u001b[1;34m(folder_path, model)\u001b[0m\n\u001b[0;32m     58\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     img \u001b[38;5;241m=\u001b[39m preprocess_image(img)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Make prediction using the model\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[1;32mIn[34], line 48\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     46\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Apply adaptive thresholding to enhance contrast\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39madaptiveThreshold(img, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mADAPTIVE_THRESH_GAUSSIAN_C, cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY_INV, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\thresh.cpp:1674: error: (-215:Assertion failed) src.type() == CV_8UC1 in function 'cv::adaptiveThreshold'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train = X_train.astype('float32') / 255.0\n",
    "    X_test = X_test.astype('float32') / 255.0\n",
    "    # Reshape images to 28x28x1 (adding a channel dimension)\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def build_model(input_shape=(28, 28, 1), num_classes=10):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(X_train, y_train, X_val, y_val):\n",
    "    model = build_model()\n",
    "    # Use early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    # Save the best model during training\n",
    "    checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=128, validation_data=(X_val, y_val), callbacks=[early_stopping, checkpoint])\n",
    "    return model\n",
    "\n",
    "def preprocess_image(img):\n",
    "    # Resize the image to 28x28 and normalize pixel values to [0, 1]\n",
    "    img = cv2.resize(img, (28, 28))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    # Apply adaptive thresholding to enhance contrast\n",
    "    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    return img\n",
    "\n",
    "def extract_numbers_from_images(folder_path, model):\n",
    "    numbers = np.zeros((9, 9), dtype=int)\n",
    "\n",
    "    for i in range(9):\n",
    "        for j in range(9):\n",
    "            img_path = os.path.join(folder_path, f\"cell[{i}][{j}].jpg\")\n",
    "            if os.path.exists(img_path):\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    img = preprocess_image(img)\n",
    "                    # Make prediction using the model\n",
    "                    prediction = model.predict(np.expand_dims(img, axis=0))\n",
    "                    # Assign the predicted digit to the corresponding position in the array\n",
    "                    numbers[i][j] = np.argmax(prediction)\n",
    "\n",
    "    return numbers\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "X_train, X_test, y_train, y_test = load_data()\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "# Train the model\n",
    "model = train_model(X_train, y_train, X_val, y_val)\n",
    "# Load the best model\n",
    "best_model = load_model(\"best_model.h5\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = \"Cells/\"\n",
    "output_array = extract_numbers_from_images(folder_path, best_model)\n",
    "print(output_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f0b36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
